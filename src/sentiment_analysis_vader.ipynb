{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7409c059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df45b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c938eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 13)\n",
      "                                  user_name    user_location  \\\n",
      "0                             DeSota Wilson      Atlanta, GA   \n",
      "1                                  CryptoND              NaN   \n",
      "2                                 Tdlmatias  London, England   \n",
      "3                      Crypto is the future              NaN   \n",
      "4  Alex Kirchmaier ðŸ‡¦ðŸ‡¹ðŸ‡¸ðŸ‡ª #FactsSuperspreader           Europa   \n",
      "\n",
      "                                    user_description         user_created  \\\n",
      "0  Biz Consultant, real estate, fintech, startups...  2009-04-26 20:05:09   \n",
      "1  ðŸ˜Ž BITCOINLIVE is a Dutch platform aimed at inf...  2019-10-17 20:12:10   \n",
      "2  IM Academy : The best #forex, #SelfEducation, ...  2014-11-10 10:50:37   \n",
      "3  I will post a lot of buying signals for BTC tr...  2019-09-28 16:48:12   \n",
      "4  Co-founder @RENJERJerky | Forbes 30Under30 | I...  2016-02-03 13:15:55   \n",
      "\n",
      "   user_followers  user_friends  user_favourites  user_verified  \\\n",
      "0          8534.0          7605             4838          False   \n",
      "1          6769.0          1532            25483          False   \n",
      "2           128.0           332              924          False   \n",
      "3           625.0           129               14          False   \n",
      "4          1249.0          1472            10482          False   \n",
      "\n",
      "                  date                                               text  \\\n",
      "0  2021-02-10 23:59:04  Blue Ridge Bank shares halted by NYSE after #b...   \n",
      "1  2021-02-10 23:58:48  ðŸ˜Ž Today, that's this #Thursday, we will do a \"...   \n",
      "2  2021-02-10 23:54:48  Guys evening, I have read this article about B...   \n",
      "3  2021-02-10 23:54:33  $BTC A big chance in a billion! Price: \\487264...   \n",
      "4  2021-02-10 23:54:06  This network is secured by 9 508 nodes as of t...   \n",
      "\n",
      "                                    hashtags               source  is_retweet  \n",
      "0                                ['bitcoin']      Twitter Web App       False  \n",
      "1  ['Thursday', 'Btc', 'wallet', 'security']  Twitter for Android       False  \n",
      "2                                        NaN      Twitter Web App       False  \n",
      "3         ['Bitcoin', 'FX', 'BTC', 'crypto']              dlvr.it       False  \n",
      "4                                    ['BTC']      Twitter Web App       False  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/chunked_data/tweets_part_1.csv\")\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9940a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset\n",
      "Index(['user_name', 'user_location', 'user_description', 'user_created',\n",
      "       'user_followers', 'user_friends', 'user_favourites', 'user_verified',\n",
      "       'date', 'text', 'hashtags', 'source', 'is_retweet'],\n",
      "      dtype='object')\n",
      "Columns after dropping some columns\n",
      "Index(['date', 'text', 'hashtags', 'is_retweet'], dtype='object')\n",
      "New shape of the dataset  (50000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in the dataset\")\n",
    "print(df.columns)\n",
    "columns_to_drop = [\n",
    "    'user_name', 'user_location', 'user_description', 'user_created',\n",
    "    'user_followers', 'user_friends', 'user_favourites', 'user_verified',\n",
    "    'source'\n",
    "] \n",
    "#remove the columns which are not needed for the analysis\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "print(\"Columns after dropping some columns\")\n",
    "print(df.columns)\n",
    "print(\"New shape of the dataset \",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3ae857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 4)\n"
     ]
    }
   ],
   "source": [
    "df = df[df['is_retweet'] == False].reset_index(drop=True)\n",
    "print(df.shape)\n",
    "\n",
    "# Deleting the duplicate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5421a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date              0\n",
      "text              0\n",
      "hashtags      10571\n",
      "is_retweet        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d72a20c",
   "metadata": {},
   "source": [
    "## Cleaning the data for VADER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "009b7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_for_vader(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
    "    # Remove @mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    # Remove hashtags \n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a98ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Blue Ridge Bank shares halted by NYSE after bi...\n",
      "1    ðŸ˜Ž Today, that's this Thursday, we will do a \"ðŸŽ¬...\n",
      "2    Guys evening, I have read this article about B...\n",
      "3    $BTC A big chance in a billion! Price: \\487264...\n",
      "4    This network is secured by 9 508 nodes as of t...\n",
      "5    ðŸ’¹ Trade Crypto on Binance ðŸ“Œ Enjoy Cashback 10%...\n",
      "6        &lt;'fire' &amp; 'man'&gt; Bitcoin Crypto BTC\n",
      "7    ðŸ”„ Prices update in $EUR (1 hour): $BTC - 37082...\n",
      "8    BTC Bitcoin Ethereum ETH Crypto cryptotrading ...\n",
      "9    .â€™s bitcoin investment is revolutionary for cr...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(preprocess_for_vader)\n",
    "\n",
    "print(df['text'].iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34b8aad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Apply language detection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlang\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetect_language\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Show all unique languages detected\u001b[39;00m\n\u001b[32m     17\u001b[39m unique_languages = df[\u001b[33m'\u001b[39m\u001b[33mlang\u001b[39m\u001b[33m'\u001b[39m].unique()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mdetect_language\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetect_language\u001b[39m(text):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m LangDetectException:\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\langdetect\\detector_factory.py:130\u001b[39m, in \u001b[36mdetect\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    128\u001b[39m detector = _factory.create()\n\u001b[32m    129\u001b[39m detector.append(text)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\langdetect\\detector.py:136\u001b[39m, in \u001b[36mDetector.detect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    133\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Detect language of the target text and return the language name\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m    which has the highest probability.\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     probabilities = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m probabilities:\n\u001b[32m    138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m probabilities[\u001b[32m0\u001b[39m].lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\langdetect\\detector.py:143\u001b[39m, in \u001b[36mDetector.get_probabilities\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_probabilities\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.langprob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_detect_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sort_probability(\u001b[38;5;28mself\u001b[39m.langprob)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\langdetect\\detector.py:161\u001b[39m, in \u001b[36mDetector._detect_block\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    159\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_lang_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngrams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    163\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._normalize_prob(prob) > \u001b[38;5;28mself\u001b[39m.CONV_THRESHOLD \u001b[38;5;129;01mor\u001b[39;00m i >= \u001b[38;5;28mself\u001b[39m.ITERATION_LIMIT:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aj281\\OneDrive\\Desktop\\crypto sentiment analysis\\crypto_sentiment_trading\\venv\\Lib\\site-packages\\langdetect\\detector.py:212\u001b[39m, in \u001b[36mDetector._update_lang_prob\u001b[39m\u001b[34m(self, prob, word, alpha)\u001b[39m\n\u001b[32m    210\u001b[39m weight = alpha / \u001b[38;5;28mself\u001b[39m.BASE_FREQ\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m xrange(\u001b[38;5;28mlen\u001b[39m(prob)):\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     prob[i] *= weight + lang_prob_map[i]\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Ensure reproducibility\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply language detection\n",
    "df['lang'] = df['text'].astype(str).apply(detect_language)\n",
    "\n",
    "# Show all unique languages detected\n",
    "unique_languages = df['lang'].unique()\n",
    "\n",
    "print(\"Unique languages detected in the dataset:\")\n",
    "print(unique_languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f570e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language frequencies in the dataset:\n",
      "lang\n",
      "en         49246\n",
      "de           149\n",
      "it            76\n",
      "nl            73\n",
      "unknown       66\n",
      "tl            56\n",
      "fr            55\n",
      "ca            35\n",
      "vi            34\n",
      "ro            25\n",
      "pt            22\n",
      "so            21\n",
      "cy            20\n",
      "hu            17\n",
      "da            15\n",
      "no            12\n",
      "es            11\n",
      "sv            11\n",
      "af            11\n",
      "tr            11\n",
      "fi             8\n",
      "id             8\n",
      "sl             5\n",
      "th             2\n",
      "sw             2\n",
      "pl             2\n",
      "hr             2\n",
      "sq             2\n",
      "sk             1\n",
      "ja             1\n",
      "ru             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def count_language_frequencies(df, lang_column='lang'):\n",
    "    return df[lang_column].value_counts()\n",
    "\n",
    "# Assuming df['lang'] has language codes\n",
    "lang_counts = count_language_frequencies(df)\n",
    "\n",
    "print(\"Language frequencies in the dataset:\")\n",
    "print(lang_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed990c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English tweets: 49246\n"
     ]
    }
   ],
   "source": [
    "def keep_only_english_tweets(df, lang_column='lang'):\n",
    "    return df[df[lang_column] == 'en'].copy()\n",
    "\n",
    "# Filtered DataFrame with only English tweets\n",
    "english_df = keep_only_english_tweets(df)\n",
    "\n",
    "# check how many remain\n",
    "print(f\"Number of English tweets: {len(english_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ce88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.to_csv('filtered_tweets_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b302f",
   "metadata": {},
   "source": [
    "## Assigning Labels to tweets Using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e40c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\aj281\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia=SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_label(text):\n",
    "    # Get the sentiment scores\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # Determine the sentiment label based on the compound score\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment counts:\n",
      "sentiment\n",
      "neutral     21604\n",
      "positive    21374\n",
      "negative     7022\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['sentiment'] = df['text'].apply(get_sentiment_label)\n",
    "# Count the number of tweets in each sentiment category \n",
    "sentiment_counts = df['sentiment'].value_counts()  \n",
    "print(\"Sentiment counts:\")\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Save the DataFrame with sentiment labels to a CSV file\n",
    "df.to_csv('tweets_with_sentiment_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
