{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61328336",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aj281\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.find('tokenizers/punkt'))# this address is used in the prepprocess_text_random_forest.py file in the nltk_data_path variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1821f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:26<00:00,  2.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocess_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-12 18:56:10</td>\n",
       "      <td>The Why Behind Microsoft’s $19 Billion Nuance ...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The Why Behind Microsoft s Billion Nuance Buy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-12 18:55:48</td>\n",
       "      <td>Make a change and an impart on yourself and ot...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Make a change and an impart on yourself and ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-12 18:55:27</td>\n",
       "      <td>What are the biggest shitcoins? crypto btc bnb...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>What are the biggest shitcoins crypto btc bnb ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-12 18:55:22</td>\n",
       "      <td>crypto bitcoin cryptocurrency blockchain btc e...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>crypto bitcoin cryptocurrency blockchain btc e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-12 18:55:01</td>\n",
       "      <td>[1D] Bitcoin market is weakly trending up curr...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Bitcoin market is weakly trending up current m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                               text  \\\n",
       "0  2021-04-12 18:56:10  The Why Behind Microsoft’s $19 Billion Nuance ...   \n",
       "1  2021-04-12 18:55:48  Make a change and an impart on yourself and ot...   \n",
       "2  2021-04-12 18:55:27  What are the biggest shitcoins? crypto btc bnb...   \n",
       "3  2021-04-12 18:55:22  crypto bitcoin cryptocurrency blockchain btc e...   \n",
       "4  2021-04-12 18:55:01  [1D] Bitcoin market is weakly trending up curr...   \n",
       "\n",
       "   is_retweet lang sentiment  \\\n",
       "0       False   en   neutral   \n",
       "1       False   en   neutral   \n",
       "2       False   en   neutral   \n",
       "3       False   en   neutral   \n",
       "4       False   en  negative   \n",
       "\n",
       "                                     preprocess_text  \n",
       "0  The Why Behind Microsoft s Billion Nuance Buy ...  \n",
       "1  Make a change and an impart on yourself and ot...  \n",
       "2  What are the biggest shitcoins crypto btc bnb ...  \n",
       "3  crypto bitcoin cryptocurrency blockchain btc e...  \n",
       "4  Bitcoin market is weakly trending up current m...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import the function from your .py file\n",
    "from preprocess_text_random_forest import run_preprocessing\n",
    "\n",
    "# Step 2: Call the function — it will return the processed DataFrame\n",
    "df = run_preprocessing()\n",
    "\n",
    "# Step 3: Now you can use df for your Random Forest or any further steps\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2f22b",
   "metadata": {},
   "source": [
    "## Vectorization of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e03b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['preprocess_text']\n",
    "y=df['sentiment']\n",
    "\n",
    "n=len(X)\n",
    "\n",
    "split_index=int(n*0.8)\n",
    "# Split the data into training and testing sets\n",
    "X_train= X[:split_index]\n",
    "X_test= X[split_index:]\n",
    "y_train= y[:split_index]\n",
    "y_test= y[split_index:]\n",
    "\n",
    "#sklearn train_test_split is not used because it splits randomly and we want to keep the order of the data for the time series modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237c94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the feature matrix X:  (576213, 167)\n",
      "Shape of the target vector Y:  (576213,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    lowercase=True, \n",
    "    max_features=10000,\n",
    "    min_df=0.01,\n",
    "    max_df=0.9,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "X_train_tfidf=vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "print(\"Shape of the feature matrix X: \",X_train_tfidf.shape)\n",
    "print(\"Shape of the target vector Y: \",y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab5ef4",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccdcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(50, 1000),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 5),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,  # Try 20 random combinations\n",
    "    cv=5,\n",
    "    scoring='accuracy',  # Change to f1_macro, roc_auc, etc. if needed\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_rf = random_search.best_estimator_\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_rf.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
